A RAG System using Open-Source LLM

A PDF-based AI Chat Assistant built with Streamlit and open-source LLMs
Leverages Retrieval-Augmented Generation (RAG) to answer user queries from PDF documents using local Llama2 models (7B & 14B).

ğŸš€ Overview

This project enables you to create an AI assistant that:

ğŸ“„ Ingests multiple PDF documents
ğŸ” Builds a vector index (FAISS) of document content
ğŸ¤– Retrieves relevant chunks based on a user query
ğŸ’¬ Generates contextual, grounded responses using an open-source LLM (Llama2)
ğŸŒ Provides a web interface via Streamlit

Itâ€™s perfect for local, privacy-focused knowledge assistants with no cloud dependency.

â­ Features
ğŸ§  Retrieval-Augmented Generation (RAG) pipeline
ğŸ“‘ PDF indexing for semantic search
ğŸ§° Uses FAISS for efficient vector search
ğŸ‘¨â€ğŸ’» Local inference with Llama2-7B / 14B models
ğŸ’¡ Simple Streamlit UI for query input and answer display
ğŸ”’ No data leaves your machine â€” full privacy



ğŸ“ How It Works

Document Processing
PDF files are read and split into text chunks.
Embeddings are created for each chunk.
Chunks are indexed in a vector database (FAISS).
Query Workflow
User enters a query in the Streamlit UI.
Similar chunks are retrieved from the FAISS index.
Rerieved chunks + user query are passed to the LLM.
LLM generates context-aware responses.

Tips & Notes

âš¡ Performance depends on your hardware (GPU recommended).
ğŸ›¡ For large corpora, consider FAISS parameters tuning.
ğŸ“Š Embedding quality & chunk size affect retrieval relevance.
